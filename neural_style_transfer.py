# -*- coding: utf-8 -*-
"""Neural_Style_Transfer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kEt_ZCONM4fhO-rl9p30JWQaNpI6fxHX

**ARMIN HAMRAH - Neural Style Transfer**

*Final Project â€“ Machine Learning for Artists*

Neural style transfer is a deep learning technique that combines the content of one image with the style of another. It uses convolutional neural networks (CNNs) to extract and separate the content and style features from input images, then synthesizes a new image that merges these elements. I find it a very interesting application of computer vision and a compelling method of creating digital art in seconds.

Special thanks to <a href="https://arxiv.org/abs/1508.06576" class="external">A Neural Algorithm of Artistic Style</a> (Gatys et al.) and TensorFlow for inspiring and guiding my neural style transfer exploration. Feel free to upload images to your Google Drive account and link it to this Colab notebook to try my model out yourself! Enjoy!
"""

#importing and configuring tools
import os
import tensorflow as tf
os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'

import IPython.display as display

import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.rcParams['figure.figsize'] = (30, 30)
mpl.rcParams['axes.grid'] = False

import numpy as np
import PIL.Image
import time
import functools

#converting tensor into a viewable image
def tensor_to_image(tensor):
  tensor = tensor*255
  tensor = np.array(tensor, dtype=np.uint8)
  if np.ndim(tensor)>3:
    assert tensor.shape[0] == 1
    tensor = tensor[0]
  return PIL.Image.fromarray(tensor)

#choosing images (this is the fun part:)

from google.colab import drive
drive.mount('/content/drive')

content_path = '/content/drive/MyDrive/Activities/Neural_Style_Transfer/venture.png'
style_path = '/content/drive/MyDrive/Activities/Neural_Style_Transfer/painting.png'

#Loading and resizing images to fit in 512 pixels
import tensorflow as tf
import matplotlib.pyplot as plt

def load_img(path_to_img):
  max_dim = 2048
  img = tf.io.read_file(path_to_img)
  img = tf.image.decode_image(img, channels=3)
  img = tf.image.convert_image_dtype(img, tf.float32)

  shape = tf.cast(tf.shape(img)[:-1], tf.float32)
  long_dim = max(shape)
  scale = max_dim / long_dim

  new_shape = tf.cast(shape * scale, tf.int32)

  img = tf.image.resize(img, new_shape)
  img = img[tf.newaxis, :]
  return img

#Creating the function to display the image
def imshow(image, title=None):
  if len(image.shape) > 3:
    image = tf.squeeze(image, axis=0)

  plt.imshow(image)
  if title:
    plt.title(title)

#Print existing photos before the grand reveal
from google.colab import drive
drive.mount('/content/drive')

content_image = load_img(content_path)
style_image = load_img(style_path)

plt.figure(figsize=(10, 10))

plt.subplot(1, 2, 1)
imshow(content_image, 'Content Image')

plt.subplot(1, 2, 2)
imshow(style_image, 'Style Image')

plt.show()

import tensorflow_hub as hub
hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')
stylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0]
tensor_to_image(stylized_image)

"""## Defining content & style representations

I use the intermediate layers of the model to extract the content and style representations of the image. Starting from the network's input layer, the first few layer activations capture low-level features like edges and textures. As we progress through the network, the final few layers reflect higher-level features, such as object components like wheels or eyes. For this project, I work with the VGG19 network architecture, a pretrained image classification network. These intermediate layers are essential for defining the representation of content and style from the images. For an input image, I aim to match the corresponding style and content target representations at these intermediate layers.
"""

#Loading in the VGG19 and testing it on image to ensure it's used correctly (https://keras.io/api/applications/vgg/#vgg19-function)
x = tf.keras.applications.vgg19.preprocess_input(content_image*255)
x = tf.image.resize(x, (224, 224))
vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')
prediction_probabilities = vgg(x)
prediction_probabilities.shape

predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]
[(class_name, prob) for (number, class_name, prob) in predicted_top_5]

predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]
print([(class_name, prob) for (number, class_name, prob) in predicted_top_5])
print("lol this is funny")

"""Now I will load a `VGG19` without the classification head, and list the layer names"""

vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')

print()
for layer in vgg.layers:
  print(layer.name)

"""Choose intermediate layers from the network to represent the style and content of the image:

"""

content_layers = ['block5_conv2']

style_layers = ['block1_conv1',
                'block2_conv1',
                'block3_conv1',
                'block4_conv1',
                'block5_conv1']

num_content_layers = len(content_layers)
num_style_layers = len(style_layers)

"""#### Intermediate layers for style and content

So why do these intermediate outputs within my pretrained image classification network allow me to define style and content representations?

At a high level, for the network to perform image classification (which this network is trained to do), it must understand the image. This involves taking the raw image as input pixels and building an internal representation that transforms those pixels into a complex understanding of the features present within the image.

This ability also explains why convolutional neural networks (CNNs) generalize well: they capture invariances and defining features within classes (e.g., cats vs. dogs) that are agnostic to background noise and other nuisances. Thus, somewhere between where the raw image is fed into the model and the output classification label, the model acts as a complex feature extractor. By accessing the intermediate layers of the model, I can describe both the content and style of input images.

## Build the model

The networks in `tf.keras.applications` are designed for dev's to easily extract the intermediate layer values using the Keras functional API.

To define a model using the functional API, I specify the inputs and outputs:

`model = Model(inputs, outputs)`

The following function builds a VGG19 model that returns a list of intermediate layer outputs:
"""

def vgg_layers(layer_names):
  """ Creates a VGG model that returns a list of intermediate output values."""
  # Load our model. Load pretrained VGG, trained on ImageNet data
  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')
  vgg.trainable = False
  outputs = [vgg.get_layer(name).output for name in layer_names]
  model = tf.keras.Model([vgg.input], outputs)
  return model

"""And to create the model:"""

style_extractor = vgg_layers(style_layers)
style_outputs = style_extractor(style_image*255)

#Look at the statistics of each layer's output
for name, output in zip(style_layers, style_outputs):
  print(name)
  print("  shape: ", output.numpy().shape)
  print("  min: ", output.numpy().min())
  print("  max: ", output.numpy().max())
  print("  mean: ", output.numpy().mean())
  print()

"""## Calculate style

The content of an image is represented by the values of the intermediate feature maps.

Interestingly, the style of an image is described by the means and correlations across the different feature maps. I calculate a Gram matrix to capture this information by taking the outer product of the feature vector with itself at each location and averaging that outer product over all locations. For a particular layer, this Gram matrix is calculated as:

$$G^l_{cd} = \frac{\sum_{ij} F^l_{ijc}(x)F^l_{ijd}(x)}{IJ}$$

I implement this calculation concisely using the `tf.linalg.einsum` function:
"""

def gram_matrix(input_tensor):
  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)
  input_shape = tf.shape(input_tensor)
  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)
  return result/(num_locations)

"""## Extract style and content

Building a model that returns the style and content tensors.
"""

class StyleContentModel(tf.keras.models.Model):
  def __init__(self, style_layers, content_layers):
    super(StyleContentModel, self).__init__()
    self.vgg = vgg_layers(style_layers + content_layers)
    self.style_layers = style_layers
    self.content_layers = content_layers
    self.num_style_layers = len(style_layers)
    self.vgg.trainable = False

  def call(self, inputs):
    "Expects float input in [0,1]"
    inputs = inputs*255.0
    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)
    outputs = self.vgg(preprocessed_input)
    style_outputs, content_outputs = (outputs[:self.num_style_layers],
                                      outputs[self.num_style_layers:])

    style_outputs = [gram_matrix(style_output)
                     for style_output in style_outputs]

    content_dict = {content_name: value
                    for content_name, value
                    in zip(self.content_layers, content_outputs)}

    style_dict = {style_name: value
                  for style_name, value
                  in zip(self.style_layers, style_outputs)}

    return {'content': content_dict, 'style': style_dict}

"""When called on an image, this model returns the gram matrix (style) of the `style_layers` and content of the `content_layers`:"""

extractor = StyleContentModel(style_layers, content_layers)

results = extractor(tf.constant(content_image))

print('Styles:')
for name, output in sorted(results['style'].items()):
  print("  ", name)
  print("    shape: ", output.numpy().shape)
  print("    min: ", output.numpy().min())
  print("    max: ", output.numpy().max())
  print("    mean: ", output.numpy().mean())
  print()

print("Contents:")
for name, output in sorted(results['content'].items()):
  print("  ", name)
  print("    shape: ", output.numpy().shape)
  print("    min: ", output.numpy().min())
  print("    max: ", output.numpy().max())
  print("    mean: ", output.numpy().mean())

"""## Run gradient descent

With this style and content extractor, I now implement the style transfer algorithm. I calculate the mean square error for my image's output relative to each target and then take the weighted sum of these losses.

Next, I set the style and content target values:
"""

style_targets = extractor(style_image)['style']
content_targets = extractor(content_image)['content']

"""Defining a `tf.Variable` to contain the image to optimize. To make this quick, I initialized it with the content image (the `tf.Variable` which is the same shape as the content image):"""

image = tf.Variable(content_image)

"""Since this is a float image, I defined a function to keep the pixel values between 0 and 1:"""

def clip_0_1(image):
  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)

"""Creating an optimizer. The paper recommends LBFGS, but Adam works okay, too:"""

opt = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)

"""To optimize this, I'm using a weighted combination of the two losses to get the total loss:"""

style_weight=1e-2
content_weight=1e4

def style_content_loss(outputs):
    style_outputs = outputs['style']
    content_outputs = outputs['content']
    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2)
                           for name in style_outputs.keys()])
    style_loss *= style_weight / num_style_layers

    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2)
                             for name in content_outputs.keys()])
    content_loss *= content_weight / num_content_layers
    loss = style_loss + content_loss
    return loss

@tf.function()
def train_step(image):
  with tf.GradientTape() as tape:
    outputs = extractor(image)
    loss = style_content_loss(outputs)

  grad = tape.gradient(loss, image)
  opt.apply_gradients([(grad, image)])
  image.assign(clip_0_1(image))

"""Now run a few steps to test:"""

train_step(image)
train_step(image)
train_step(image)
tensor_to_image(image)

"""Since it's working, perform a longer optimization:"""

import time
start = time.time()

epochs = 10
steps_per_epoch = 100

step = 0
for n in range(epochs):
  for m in range(steps_per_epoch):
    step += 1
    train_step(image)
    print(".", end='', flush=True)
  display.clear_output(wait=True)
  display.display(tensor_to_image(image))
  print("Train step: {}".format(step))

end = time.time()
print("Total time: {:.1f}".format(end-start))

"""## Total variation loss

One downside to this basic implementation is that it produces a lot of high frequency artifacts. I decreased these using an explicit regularization term on the high frequency components of the image. In style transfer, this is known as the *total variation loss*:
"""

def high_pass_x_y(image):
  x_var = image[:, :, 1:, :] - image[:, :, :-1, :]
  y_var = image[:, 1:, :, :] - image[:, :-1, :, :]

  return x_var, y_var

x_deltas, y_deltas = high_pass_x_y(content_image)

plt.figure(figsize=(14, 10))
plt.subplot(2, 2, 1)
imshow(clip_0_1(2*y_deltas+0.5), "Horizontal Deltas: Original")

plt.subplot(2, 2, 2)
imshow(clip_0_1(2*x_deltas+0.5), "Vertical Deltas: Original")

x_deltas, y_deltas = high_pass_x_y(image)

plt.subplot(2, 2, 3)
imshow(clip_0_1(2*y_deltas+0.5), "Horizontal Deltas: Styled")

plt.subplot(2, 2, 4)
imshow(clip_0_1(2*x_deltas+0.5), "Vertical Deltas: Styled")

"""This shows how the high frequency components have increased.

Also, this high frequency component is basically an edge-detector. I got a similar output from the Sobel edge detector, for example:
"""

plt.figure(figsize=(14, 10))

sobel = tf.image.sobel_edges(content_image)
plt.subplot(1, 2, 1)
imshow(clip_0_1(sobel[..., 0]/4+0.5), "Horizontal Sobel-edges")
plt.subplot(1, 2, 2)
imshow(clip_0_1(sobel[..., 1]/4+0.5), "Vertical Sobel-edges")

"""The regularization loss associated with this is the sum of the squares of the values:"""

plt.figure(figsize=(14, 10))

sobel = tf.image.sobel_edges(content_image)
plt.subplot(1, 2, 1)
imshow(clip_0_1(sobel[..., 0]/4+0.5), "Horizontal Sobel-edges")
plt.subplot(1, 2, 2)
imshow(clip_0_1(sobel[..., 1]/4+0.5), "Vertical Sobel-edges")

total_variation_loss(image).numpy()

"""That demonstrated what it does. But there's no need for me to implement it yourself, since TensorFlow includes a standard implementation:"""

tf.image.total_variation(image).numpy()

"""## Re-run the optimization

I chose a weight of 30 for the `total_variation_loss`, though this can be changed:
"""

total_variation_weight=30

"""Now including it in the `train_step` function:"""

@tf.function()
def train_step(image):
  with tf.GradientTape() as tape:
    outputs = extractor(image)
    loss = style_content_loss(outputs)
    loss += total_variation_weight*tf.image.total_variation(image)

  grad = tape.gradient(loss, image)
  opt.apply_gradients([(grad, image)])
  image.assign(clip_0_1(image))

"""Reinitializing the image-variable and the optimizer:"""

opt = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)
image = tf.Variable(content_image)

"""And running the optimization:"""

import time
start = time.time()

epochs = 10
steps_per_epoch = 100

step = 0
for n in range(epochs):
  for m in range(steps_per_epoch):
    step += 1
    train_step(image)
    print(".", end='', flush=True)
  display.clear_output(wait=True)
  display.display(tensor_to_image(image))
  print("Train step: {}".format(step))

end = time.time()
print("Total time: {:.1f}".format(end-start))

"""Finally, saving the result!"""

file_name = 'stylized-image.png'
tensor_to_image(image).save(file_name)

try:
  from google.colab import files
  files.download(file_name)
except (ImportError, AttributeError):
  pass